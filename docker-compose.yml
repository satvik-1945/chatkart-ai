version: "3.9"

services:
  rasa:
    image: rasa/rasa:3.6.21
    container_name: rasa_server
    volumes:
      - ./:/app
      - ./models:/app/models
    ports:
      - "5005:5005"
    command: >
      run --enable-api --cors "*"

  actions:
    build:
      context: .
      dockerfile: Dockerfile.actions
    container_name: rasa_actions
    volumes:
      - ./actions:/app/actions
    ports:
      - "5055:5055"

  llm_service:
    build:
      context: ./llm_service
      dockerfile: Dockerfile
    image: satvik1945/llm_service:latest # Make sure this matches your Docker Hub username
    container_name: llm_service
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_API_URL=http://ollama_server:11434
      - DEFAULT_LLM_MODEL=deepseek-r1:1.5b
    depends_on:
      - ollama_server

  ollama_server: # OLLAMA SERVICE
    image: ollama/ollama:latest
    container_name: ollama_server
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      # - ./models_custom:/app/models_custom # Uncomment this if you're using custom local GGUF models
    # REMOVE THIS ENTIRE 'deploy' SECTION FOR MAC M3
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  mongo:
    image: mongo:6.0
    container_name: mongo_db
    restart: always
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db

volumes:
  mongo_data:
  ollama_data: